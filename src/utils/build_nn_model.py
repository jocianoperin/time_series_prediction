layers_cfg = [
    # 1ï¸âƒ£ LSTM bidirecional capta padrÃµes de longo prazo
    {"type": "LSTM", "units": 50, "bidirectional": False,
     "return_sequences": True, "activation": "relu"},

    # 2ï¸âƒ£ LSTM unidirecional para compressÃ£o de sequÃªncia
    {"type": "LSTM", "units": 25, "bidirectional": False,
     "return_sequences": False, "activation": "relu"},

    # 3ï¸âƒ£ Dense final para regressÃ£o
    {"type": "Dense", "units": 1}
]
"""
layers_cfg = [
    # 1ï¸âƒ£ LSTM bidirecional capta padrÃµes de longo prazo
    {"type": "LSTM", "units": 128, "bidirectional": True,
     "return_sequences": True, "dropout": 0.1},

    # 2ï¸âƒ£ GRU unidirecional reduz parÃ¢metros e mantÃ©m sequÃªncia
    {"type": "GRU",  "units": 64,
     "return_sequences": True, "dropout": 0.1},

    # 3ï¸âƒ£ Bloco de atenÃ§Ã£o multiâ€‘head (heads * key_dim â‰ˆ units da GRU)
    {"type": "ATTN", "heads": 2, "key_dim": 16, "dropout": 0.1},

    # 4ï¸âƒ£ GRU final para condensar a sequÃªncia
    {"type": "GRU",  "units": 64, "return_sequences": False, "dropout": 0.2},

    # 5ï¸âƒ£ Dense intermediÃ¡ria
    {"type": "Dense","units": 16, "activation": "relu", "dropout": 0.1}
]
layers_cfg = [
    # hÃ­brido um pouco maior
    {"type": "GRU",  "units": 96, "bidirectional": True, "return_sequences": True},
    {"type": "ATTN", "heads": 3,  "key_dim": 24, "dropout": 0.1},
    {"type": "LSTM", "units": 64, "return_sequences": False, "dropout": 0.1},
    {"type": "Dense","units": 32, "activation": "relu", "dropout": 0.1},
]"""

# ----------------------------------------------------------------------
# ğŸ“š  EXPLICAÃ‡ÃƒO DETALHADA DE `layers_cfg`
#
# Cada dicionÃ¡rio nesta lista descreve **um bloco de rede** que serÃ¡ criado
# dinamicamente pela funÃ§Ã£o `build_lstm_model()`.  Os campos aceitos sÃ£o:
#
#   â€¢ type               â†’  "LSTM", "GRU", "DENSE" ou "ATTN"
#   â€¢ units              â†’  NÂº de neurÃ´nios (somente LSTM / GRU / Dense)
#   â€¢ bidirectional      â†’  Usa wrapper `tf.keras.layers.Bidirectional`
#   â€¢ return_sequences   â†’  MantÃ©m 3â€‘D na saÃ­da (obrigatÃ³rio p/ ATTN depois)
#   â€¢ activation         â†’  FunÃ§Ã£o de ativaÃ§Ã£o na cÃ©lula (padrÃ£o: "tanh")
#   â€¢ recurrent_activationâ†’ FunÃ§Ã£o porta (padrÃ£o: "sigmoid")
#   â€¢ dropout            â†’  Dropout aplicado **apÃ³s** a camada (exceto ATTN)
#   â€¢ heads, key_dim     â†’  NÂº de cabeÃ§as e dimensÃ£oâ€‘chave p/ Multiâ€‘Head Attention
#
# â†“ ComentÃ¡rio linhaâ€‘aâ€‘linha do bloco escolhido â†“
# ----------------------------------------------------------------------
# 1ï¸âƒ£  LSTM bidirecional
#     â€¢ units=128  â†’  128 neurÃ´nios na direÃ§Ã£o forward **e** 128 na backward.
#       â†’  SaÃ­da tem 256 features por passo.  Ex.: input (B, 7, 9) â†’ output (B, 7, 256)
#     â€¢ return_sequences=True  â†’  MantÃ©m dimensÃ£o temporal para a GRU seguinte.
#     â€¢ dropout=0.1  â†’  10â€¯% das ativaÃ§Ãµes zeradas p/ reduzir overfitting.
#
# 2ï¸âƒ£  GRU unidirecional
#     â€¢ units=64    â†’  64 neurÃ´nios; saÃ­da = (B, 7, 64)
#     â€¢ return_sequences=True  â†’  NecessÃ¡rio porque a atenÃ§Ã£o recebe tensor 3â€‘D.
#     â€¢ dropout=0.1  â†’  Dropout pÃ³sâ€‘GRU (nÃ£o afeta estado interno).
#
# 3ï¸âƒ£  Multiâ€‘Head Attention
#     â€¢ heads=2, key_dim=16  â†’  2 cabeÃ§as paralelas, cada uma projeta Q/K/V
#       em 16 dimensÃµes.  Custo: O(heads * seq_lenÂ² * key_dim).
#       Ex.: seq_len=7  â†’  matrizes 7Ã—7 pequeninas; custo irrisÃ³rio.
#     â€¢ dropout=0.1  â†’  Dropout dentro da prÃ³pria atenÃ§Ã£o.
#     â€¢ O bloco adiciona residual + LayerNorm:  `out = LayerNorm(x + Attn(x))`.
#
# 4ï¸âƒ£  GRU final (compressÃ£o)
#     â€¢ units=64, return_sequences=False  â†’  Converte (B, 7, 64) em (B, 64)
#       pegando apenas o Ãºltimo passo temporal (saÃ­da vetorial).
#     â€¢ dropout=0.2  â†’  Aplicado no vetor (B, 64) antes da Dense.
#
# 5ï¸âƒ£  Dense intermediÃ¡ria
#     â€¢ units=16, activation="relu" â†’  Extrai combinaÃ§Ãµes nÃ£oâ€‘lineares
#       do vetor de 64 features â†’ (B, 16).
#     â€¢ dropout=0.1  â†’  Ãšltima regularizaÃ§Ã£o antes da saÃ­da linear.
#
# â†’  SaÃ­da final da funÃ§Ã£o `build_lstm_model()`:
#        (B, 1)   â€”  camada `Dense(1, activation="linear")` para regressÃ£o.
#
# Exemplos prÃ¡ticos
# -----------------
# â–¸ Se TIME_STEPS = 7, n_features = 9 e batch_size = 32:
#     Input:  (32, 7, 9)
#     LSTM   â†’ (32, 7, 256)
#     GRU    â†’ (32, 7, 64)
#     ATTN   â†’ (32, 7, 64)  (heads*key_dim == 32, broadcast p/ 64 via concat)
#     GRU    â†’ (32, 64)
#     Dense  â†’ (32, 16)
#     Out    â†’ (32, 1)
#
# â–¸ Para ativar bidirecional tambÃ©m na primeira GRU bastaria
#     {"type": "GRU", "units": 64, "bidirectional": True, ...}
#
# â–¸ Para prever mÃºltiplos dias de uma vez (horizonte=7), troque
#     outputs = Dense(7, activation="linear")(x)
#   e ajuste a funÃ§Ã£o de perda para `tf.keras.losses.MAE` com shape (B, 7).
#
# Mantendo esses comentÃ¡rios no arquivo vocÃª documenta o racional da arquitetura
# e facilita ajustes futuros (tuning de unidades, heads, dropout, etc.).
# ----------------------------------------------------------------------