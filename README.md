# ğŸ“ˆ PrevisÃ£o de SÃ©ries Temporais (time_series_prediction)

Projeto desenvolvido por **Jociano Perin** como parte do seu mestrado profissional em InteligÃªncia Computacional na **UTFPR (Universidade TecnolÃ³gica Federal do ParanÃ¡)**. 

Este repositÃ³rio contÃ©m todo o pipeline ajustado para previsÃ£o de sÃ©ries temporais com base no conjunto de dados reais utilizados no projeto, considerando o perÃ­odo de **01/01/2019 a 31/12/2023**, com foco em produtos identificados por cÃ³digo de barras.

O projeto foi desenvolvido para realizar previsÃµes robustas utilizando modelos de Machine Learning (ML), com foco especial em Redes Neurais (LSTM) e XGBoost.

## ğŸ” O que este projeto faz?

- **PreparaÃ§Ã£o de dados:** Realiza o prÃ©-processamento completo dos dados.
- **Feature Engineering:** Cria automaticamente diversas features temporais como:
  - Dia da semana, mÃªs, se Ã© fim de semana, feriados.
  - Lags e mÃ©dias mÃ³veis configurÃ¡veis.
- **Modelos Implementados:**
  - XGBoost
  - Redes Neurais (LSTM)
- **ValidaÃ§Ã£o e AvaliaÃ§Ã£o:** Avalia os modelos usando mÃ©tricas como MAE, RMSE, MAPE e SMAPE.
- **VisualizaÃ§Ã£o:** Gera grÃ¡ficos comparativos entre previsÃµes e dados reais.

---

## ğŸ“‚ Estrutura do projeto

```bash
time_series_prediction/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ <barcode>.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ train_nn.py
â”‚   â”œâ”€â”€ train_xgboost.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ build_nn_model.py  # define a funÃ§Ã£o build_lstm_model
â”‚       â”œâ”€â”€ logging_config.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

- `data/<barcode>.csv`: Dados brutos utilizados para previsÃ£o.
- `src/feature_engineering.py`: Script responsÃ¡vel por criar e manipular novas features.
- `src/train_nn.py`: ImplementaÃ§Ã£o das Redes Neurais (LSTM).
- `src/train_xgboost.py`: ImplementaÃ§Ã£o do modelo XGBoost.
- `src/utils.py`: FunÃ§Ãµes auxiliares e mÃ©tricas de avaliaÃ§Ã£o.
- `src/main.py`: Script principal para execuÃ§Ã£o do pipeline.

## ğŸ—‚ï¸ Estrutura dos Dados (.csv)

Este projeto espera que os arquivos de dados estejam no diretÃ³rio:

```
data/raw/
```

Cada arquivo deve estar nomeado com o cÃ³digo de barras (barcode) do produto, no formato:

```
<barcode>.csv
```

### ğŸ“„ Estrutura esperada do arquivo `.csv`:

```csv
Date,Barcode,OnPromotion,Quantity,ReturnedQuantity,Discount,Increase,TotalValue,UnitValue,CostValue,Holiday
2019-01-02,7891021006125,0,17.0,0.0,0.946,0.0,169.15,9.95,8.66,0
2019-01-03,7891021006125,0,30.0,0.0,0.884,0.0,298.5,9.95,8.66,0
...
```

### ğŸ•“ PerÃ­odo de dados padrÃ£o

O projeto foi ajustado para operar com dados entre:

```
01/01/2019 a 31/12/2023
```

Esse intervalo foi definido com base na disponibilidade e qualidade dos dados reais utilizados na pesquisa.

---

## ğŸš€ Como configurar e executar o projeto (Conda + Python 3.10)

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/jocianoperin/time_series_prediction.git
cd time_series_prediction
```

### 2. Crie e ative o ambiente Conda

```bash
conda env create -f environment.yml
conda activate tsenv
```


### 3. Execute o pipeline principal

```bash
python src/main.py
```

---

## ğŸ› ï¸ Pipeline Principal (`src/main.py`)

Este script foi reorganizado para:

- **Controle de concorrÃªncia** usando `multiprocessing` e semÃ¡foros:
    - **MAX_PARALLEL_PROCS:** processos simultÃ¢neos (e.g. 4)  
    - **MAX_XGB_CONCURRENT:** XGBoost concorrentes (GPU leve)  
    - **NN_GPU_LOCK:** GPU exclusiva para Redes Neurais  
- **OtimizaÃ§Ãµes de GPU/BLAS:**

    ```python
    os.environ["OMP_NUM_THREADS"]       = "2"
    os.environ["OPENBLAS_NUM_THREADS"]  = "2"
    ```

- **Fluxo de processamento** por produto:
    1. Leitura e conversÃ£o de datas  
    2. Feature engineering (`create_features`)  
    3. Treino XGBoost em paralelo  
    4. Treino NN (LSTM/GRU/ATTN) em exclusividade de GPU  
    5. ConsolidaÃ§Ã£o de previsÃµes e mÃ©tricas  
    6. Movimento dos CSVs processados para `data/processed/`

## ğŸ› ï¸ Adicionando Features

Para adicionar novas features ao projeto:

1. Abra o arquivo `src/feature_engineering.py`.
2. Adicione sua lÃ³gica no mÃ©todo `create_features(df)`.
   
Exemplo:

```python
df["nova_feature"] = df["feature_existente"].shift(7)  # exemplo de nova feature de lag
```

---

## ğŸ§  Modificando as camadas da Rede Neural (NN)

A definiÃ§Ã£o da arquitetura estÃ¡ agora isolada em `src/utils/build_nn_model.py`. Para alterar:

1. Abra o arquivo `src/utils/build_nn_model.py`.  
2. Localize a variÃ¡vel `layers_config`, que Ã© uma lista de dicionÃ¡rios com as suas camadas.  
3. Adicione, remova ou edite cada entry conforme desejado.  
4. Salve e execute o pipeline (`python src/main.py` ou `python src/train_nn.py`) para validar.

### Exemplo original de `layers_config`

```python
layers_cfg = [
    {"type": "LSTM",   "units": 128, "activation": "relu", "dropout": 0.1, "return_sequences": True,  "bidirectional": True},
    {"type": "LSTM",   "units": 64,  "activation": "relu", "dropout": 0.1, "return_sequences": False},
    {"type": "Dense",  "units": 32,  "activation": "relu", "dropout": 0.1},
]
```

Exemplo com camada adicional:

```python
layers_cfg = [
    {"type": "LSTM",   "units": 128, "activation": "relu", "dropout": 0.1, "return_sequences": True,  "bidirectional": True},
    {"type": "LSTM",   "units": 64,  "activation": "relu", "dropout": 0.1, "return_sequences": True},
    {"type": "LSTM",   "units": 32,  "activation": "relu", "dropout": 0.1},
    {"type": "Dense",  "units": 32,  "activation": "relu", "dropout": 0.1},
]
```

Camada 1 removida (a de 512 unidades)

```python
#{"type": "LSTM", "units": 512, "activation": "relu", "dropout": 0.4, "return_sequences": True, "bidirectional": True},
# Nova 1: LSTM com 256 unidades, bidirecional, permanece a mesma
#{"type": "LSTM", "units": 256, "activation": "relu", "dropout": 0.1, "return_sequences": True, "bidirectional": True},
# A camada GRU permanece
#{"type": "GRU",  "units": 128, "activation": "relu", "dropout": 0.1, "return_sequences": True},
# Camada de atenÃ§Ã£o permanece
#{"type": "ATTN", "heads": 4,   "key_dim": 32, "dropout": 0.1},
# Camada 5 removida (a de 64 unidades)
#{"type": "LSTM", "units": 64,  "activation": "relu", "dropout": 0.2, "return_sequences": False},
# Camada final densa permanece
#{"type": "Dense","units": 32,  "activation": "relu", "dropout": 0.1},
```

---

### âš™ï¸ HiperparÃ¢metros do XGBoost

O modelo XGBoost utiliza os seguintes hiperparÃ¢metros, otimizados para sÃ©ries temporais de vendas com padrÃ£o estÃ¡vel e ruÃ­do moderado:

```python
params = {
    "objective": "reg:squarederror",   # RegressÃ£o com erro quadrÃ¡tico
    "eval_metric": "mae",              # MÃ©trica mais robusta contra outliers
    "learning_rate": 0.01,             # Aprendizado mais lento e estÃ¡vel
    "max_depth": 6,                    # Profundidade equilibrada para generalizaÃ§Ã£o
    "subsample": 0.8,                  # Amostragem parcial evita overfitting
    "colsample_bytree": 0.8,           # SeleÃ§Ã£o parcial de features por Ã¡rvore
    "min_child_weight": 3,             # Folhas mÃ­nimas com 3 instÃ¢ncias (evita splits fracos)
    "gamma": 0.1,                      # Exige ganho mÃ­nimo para split (regularizaÃ§Ã£o)
    "lambda": 1.0,                     # RegularizaÃ§Ã£o L2 nos pesos
    "tree_method": "hist",             # Treinamento otimizado via histogramas
    "device": "cuda",                  # Utiliza GPU com suporte CUDA
    "verbosity": 0,                    # Executa de forma silenciosa
    "seed": 42                         # Reprodutibilidade garantida
}
```

#### ğŸ” RecomendaÃ§Ãµes de ajuste por tipo de sÃ©rie:

| Tipo de sÃ©rie                      | SugestÃµes de ajuste                                              |
|-----------------------------------|------------------------------------------------------------------|
| **Alta volatilidade**             | Aumentar `min_child_weight` (ex: 5), reduzir `max_depth` (ex: 3) |
| **SÃ©ries com tendÃªncias sazonais**| Aumentar `max_depth` (ex: 8), manter `subsample` e `colsample_bytree` altos |
| **SÃ©ries muito curtas**           | Reduzir `min_child_weight`, usar `max_depth` menor (ex: 2â€“3)     |
| **SÃ©ries com muitos outliers**    | Trocar `eval_metric` para `"quantile"` (requer ajustes extras)  |


## ğŸ“ˆ Avaliando os Resultados

ApÃ³s execuÃ§Ã£o, o script gera grÃ¡ficos comparativos e exibe no terminal as mÃ©tricas de desempenho:

```bash
MAE: 123.45
RMSE: 234.56
MAPE: 12.34%
SMAPE: 13.45%
```

---

## ğŸ“Œ ConsideraÃ§Ãµes finais

- Certifique-se de que seu dataset esteja no formato correto em `data/vendas.csv`.
- Utilize o notebook disponÃ­vel em `notebooks/analysis.ipynb` para realizar anÃ¡lises detalhadas e exploraÃ§Ãµes adicionais.

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob licenÃ§a [MIT License](LICENSE).
